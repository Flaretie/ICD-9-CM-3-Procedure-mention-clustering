{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 解决matplotlib以及seaborn中文字方块的问题\n",
    "from pylab import mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# 3D plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "# 分词方程： seperater\n",
    "import jieba\n",
    "import re\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      10216\n",
       "unique      9658\n",
       "top       耳硬化分离术\n",
       "freq           3\n",
       "Name: detail_name, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# icd9-cm3 导入\n",
    "icd9v2 = pd.read_csv(r'~\\Desktop\\icd9_cm3\\resource\\icd9_cm3.csv',encoding = 'gb18030')\n",
    "icd9v3 = icd9v2[icd9v2['detail_name'] != '(null)']\n",
    "icd9v3 = icd9v3.reset_index(drop=True)\n",
    "icd9v3['detail_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_guangzhou = pd.read_excel(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\resource\\广东省ICD-9-CM-3手术与操作分类代码-20180101.xls',encoding = 'gb18030')\n",
    "icd9_beijing = pd.read_excel(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\resource\\北京版ICD－9－DM－3为基础的手术代码 Microsoft Excel 工作表.xls',encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_guangzhou = icd9_guangzhou.rename(index=str, columns={\"fopname\": \"detail_name\"})\n",
    "icd9_beijing = icd9_beijing.rename(index=str, columns={\"描述\": \"detail_name\"})\n",
    "\n",
    "seg_df = icd9v3[['detail_name']]\n",
    "seg_df = seg_df.append(icd9_guangzhou[[\"detail_name\"]])\n",
    "seg_df = seg_df.append(icd9_beijing[[\"detail_name\"]])\n",
    "\n",
    "seg_df = seg_df.drop_duplicates(keep = 'first')\n",
    "seg_df = seg_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 制作字典\n",
    "- 目标：合并原先被错误分词的词语，去掉没有意义的高频次\n",
    "- 资源来源\n",
    "    - 北京大学云中心的“提交-字典数据”\n",
    "    - [王晔](wangye@wondersgroup.com)提供的“组织、部位与治疗方式.xlsx与新词探查工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYTh_\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py:329: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 已知字典导入\n",
    "dirc = pd.read_csv(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\resource\\医学类词库.txt', engine = \"python\" ,encoding = 'utf-8', header = None)\n",
    "body_part = pd.read_excel(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\resource\\组织、部位与治疗方式.xlsx',encoding = 'gb18030')\n",
    "body_part2 = pd.read_excel(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\resource\\提交-词典数据.xlsx',encoding = 'gb18030',sheetname = '手术及治疗')\n",
    "# 未知字典制作：通过正反向最大词向量在原数据中发现新词\n",
    "fs = pd.read_csv(r'~\\Desktop\\icd9_cm3\\resource\\forward.csv',encoding = 'utf-8', header = [-1])\n",
    "bs = pd.read_csv(r'~\\Desktop\\icd9_cm3\\resource\\backward.csv',encoding = 'utf-8', header = [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsbox = []\n",
    "\n",
    "for word in fs[0]:\n",
    "    temp = word.split(' ')\n",
    "    for i in range(len(temp)):\n",
    "#        if len(temp[i]) == 1 or len(temp[i]) > 6:   #只选取字符长度 < = 5 的术式名称\n",
    "#            continue\n",
    "#        else:\n",
    "            fsbox.append(temp[i])\n",
    "\n",
    "bsbox = []\n",
    "\n",
    "for word in bs[0]:\n",
    "    temp = word.split(' ')\n",
    "    for i in range(len(temp)):\n",
    "        bsbox.append(temp[i])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#字典建立\n",
    "dictionary = []\n",
    "\n",
    "#导入第一本字典\n",
    "for i in range(len(body_part.columns)):\n",
    "    temp = body_part[body_part.columns[i]].dropna()    \n",
    "    for j in range(len(temp)):\n",
    "        dictionary.append(temp[j])\n",
    "\n",
    "#导入第二本字典\n",
    "for i in range(len(body_part2['词语'])):\n",
    "    if len(str(body_part2['词语'][i])) <  5:\n",
    "        dictionary.append(len(body_part2['词语'][i]))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# 导入正向查找字典\n",
    "for word in fsbox:\n",
    "    dictionary.append(word)\n",
    "    \n",
    "# 导入反向查找字典\n",
    "for word in bsbox:\n",
    "    dictionary.append(word)\n",
    "    \n",
    "# 导入医学字典\n",
    "for word in dirc[0].tolist():\n",
    "    dictionary.append(word)\n",
    "\n",
    "np.savetxt(r'C:\\Users\\MYTh_\\Desktop\\wyfc_2015\\dictionary.txt', dictionary, fmt='%s',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同结构的字典，为了后续的分析而建立\n",
    "list_dic = []\n",
    "\n",
    "for word in dictionary:\n",
    "    list_dic.append(re.findall('[\\u4E00-\\u9FA5A-Za-z]+',str(word)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到并保存去掉不需要字符且没有被分割的手术名称\n",
    "\n",
    "for i in range(len(icd9v3.detail_name)):      \n",
    "    hans = re.findall('[\\u4E00-\\u9FA5A-Za-z]+',icd9v3.detail_name[i])\n",
    "    combine = ''.join(hans)\n",
    "    icd9v3.loc[i, 'clean_dn'] = combine\n",
    "\n",
    "np.savetxt(r'C:\\Users\\MYTh_\\Desktop\\wyfc_2015\\test.txt', icd9v3.clean_dn, fmt='%s',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MYTh_\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.732 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "for word in list_dic:\n",
    "    jieba.suggest_freq(word, True)   #导入字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 分词方程： seperater\n",
    "def seperater(param):\n",
    "        \n",
    "    hans_before = \"\".join(re.findall('[\\u4E00-\\u9FA5A-Za-z]+',param))\n",
    "    hans_after = re.sub(u\"\\,\\'|(术+)|(的+)|(病+)\",\"\",hans_before)\n",
    "    after_seg = ''.join(hans_after)\n",
    "    \n",
    "    seg_list = jieba.cut(after_seg, cut_all=False)\n",
    "    temp = \" \".join(seg_list)\n",
    "\n",
    "    return temp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 执行分词并保存\n",
    "for i in range(len(icd9v3)):\n",
    "    icd9v3.loc[i, 'tokens'] = seperater(icd9v3.detail_name[i])\n",
    "\n",
    "for i in range(len(seg_df)):\n",
    "    seg_df.loc[i, 'tokens'] = seperater(seg_df.detail_name[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\result\\segment.txt', seg_df, fmt='%s',encoding = 'utf-8')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征词提取\n",
    "一般情况下，名词和动词比其他词性的词重要。另外，词语包括的字数越多，包含的信息量越大。论文中定义了一种基于词性和词长度的特征词权重计算公式，即\n",
    "                        $Weight{w_{id}} = \\lambda Weight_{pos}(w_{id})+(1-\\lambda)Weight_{len}(w_{id})$\n",
    "式中\n",
    "- $Weight(w_{id})$表示词语$w_i$在文本$d$中的权重\n",
    "- $Weight_{pos}(w_{id})$表示$w_i$在文本d中的词性权重\n",
    "- $Weight_{len}(w_{id})$表示$w_i$在文本d中的长度权重\n",
    "- $\\lambda$和$(1-\\lambda)$为加权系数，$\\lambda$取经验值0.6\n",
    "- $Weight_{pos}(w_{id})$和$Weight_{len}(w_{id})$的具体计算公式为\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 权重分词 \n",
    "posdf = pd.DataFrame()\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "for word in list_dic:\n",
    "    jieba.suggest_freq(word, True)\n",
    "\n",
    "for i in range(len(icd9v3.clean_dn)):\n",
    "    words = pseg.cut(icd9v3.clean_dn[i])\n",
    "    for word1,j in zip(words,range(16)):\n",
    "        posdf.loc[i,j] = '%s'%word1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYTh_\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "# 词向量训练\n",
    "model = Word2Vec(sentences = LineSentence(r'~\\Desktop\\icd9_cm3\\result\\segment.txt'),\n",
    "                 size = 300, \n",
    "                 min_count = 2,\n",
    "                 sg=1)  \n",
    "model.save('op_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 高频词查找\n",
    "freq_count = []\n",
    "\n",
    "for w in model.wv.vocab:\n",
    "        freq_count.append([w,model.wv.vocab[w].count])\n",
    "token_freq = pd.DataFrame(data = freq_count, columns = ['token_name','frequency'])\n",
    "np.savetxt(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\result\\high_freq.txt', token_freq.sort_values(by = [\"frequency\"],ascending = False), fmt='%s',encoding = 'utf-8')\n",
    "token_freq = token_freq.sort_values(by = [\"frequency\"],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_writer = pd.ExcelWriter(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\high_freq.xlsx')\n",
    "token_freq.sort_values(by = [\"frequency\"],ascending = False).to_excel(hf_writer,'Sheet1')\n",
    "hf_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYTh_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 训练范围：全部词\n",
    "# 求出每个手术名称的词向量均值\n",
    "vector_list = []\n",
    "\n",
    "for i in range(len(icd9v3.tokens)):\n",
    "    vector_list.append(np.mean(model[list(icd9v3.tokens[i].split(' '))], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYTh_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 训练范围：匹配词\n",
    "# 求出每个手术名称的词向量均值\n",
    "vector_list = []\n",
    "\n",
    "for i in range(len(matched_tokens)):\n",
    "    vector_list.append(np.mean(model[list(matched_tokens[i].split(' '))], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k均值聚类\n",
    "# 聚类数量：默认4000；如果需要测试匹配词需要调低\n",
    "kmeans = cluster.KMeans(n_clusters = 4000 ,max_iter = 500)\n",
    "kmeans.fit(vector_list)\n",
    "labels= kmeans.predict(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 标记所有手术名称并保存\n",
    "icd9v3[\"label\"] = labels\n",
    "icd9v3_sorted = icd9v3.sort_values(by = ['label'], ascending = (True))\n",
    "labeled_icd9 = icd9v3_sorted[['detail_code','detail_name','label']]\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\result\\labeled_ICD9-CM3_test.xlsx')\n",
    "labeled_icd9.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试代码\n",
    "1. 匹配词分类查看\n",
    "2. 匹配词计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匹配词单独分类文件保存\n",
    "\n",
    "matched = pd.DataFrame()\n",
    "matched['sp_names'] = matched_tokens\n",
    "matched[\"label\"] = labels\n",
    "\n",
    "matched = matched.sort_values(by = ['label'], ascending = (True))\n",
    "\n",
    "for i in range(len(matched)):\n",
    "    matched.loc[i,'detail_name'] = \"\".join(matched.sp_names[i].split(' '))\n",
    "\n",
    "cleandf = pd.merge(matched, icd9v3, how = 'left' ,left_on = ['sp_names'], right_on = ['tokens'])\n",
    "clean = cleandf[['detail_code','detail_name_x','label_y']]\n",
    "clean = clean.sort_values(by = ['label_y'], ascending = (True))\n",
    "clean = clean.drop_duplicates()\n",
    "clean = clean.reset_index(drop = True)    \n",
    "    \n",
    "writer = pd.ExcelWriter(r'C:\\Users\\MYTh_\\Desktop\\icd9_cm3\\result\\match-labeled_ICD9-CM3.xlsx')\n",
    "clean.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "匹配成功的分词有：8821\n",
      "匹配失败的分词有：1395\n"
     ]
    }
   ],
   "source": [
    "# 匹配/不匹配词计数\n",
    "\n",
    "matched_tokens = []\n",
    "unmatched_tokens= []\n",
    "\n",
    "for i in range(len(icd9v3.tokens)):\n",
    "    temp = icd9v3.tokens[i].split(' ')\n",
    "    for word in temp:\n",
    "        if word in dictionary:\n",
    "            matched_tokens.append(icd9v3.tokens[i])\n",
    "            break\n",
    "        else:\n",
    "            unmatched_tokens.append(icd9v3.tokens[i])\n",
    "            break  \n",
    "            \n",
    "print(\"匹配成功的分词有：\" + str(len(matched_tokens)))\n",
    "print(\"匹配失败的分词有：\" + str(len(unmatched_tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
